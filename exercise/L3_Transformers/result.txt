###############################################
result for two tests: batch_size = 16, 32
###############################################

***** Running training *****
  Num examples = 104743
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 32735
{'loss': 0.6845, 'learning_rate': 1.9694516572475945e-05, 'epoch': 0.08}                                           
{'loss': 0.6696, 'learning_rate': 1.9389033144951888e-05, 'epoch': 0.15}                                           
{'loss': 0.6627, 'learning_rate': 1.908354971742783e-05, 'epoch': 0.23}                                            
{'loss': 0.6592, 'learning_rate': 1.8778066289903775e-05, 'epoch': 0.31}                                           
{'loss': 0.6589, 'learning_rate': 1.8472582862379718e-05, 'epoch': 0.38}                                           
{'loss': 0.6577, 'learning_rate': 1.816709943485566e-05, 'epoch': 0.46}                                            
{'loss': 0.6552, 'learning_rate': 1.7861616007331604e-05, 'epoch': 0.53}                                           
{'loss': 0.6546, 'learning_rate': 1.7556132579807548e-05, 'epoch': 0.61}                                           
{'loss': 0.6576, 'learning_rate': 1.725064915228349e-05, 'epoch': 0.69}                                            
{'loss': 0.6543, 'learning_rate': 1.6945165724759434e-05, 'epoch': 0.76}                                           
{'loss': 0.6534, 'learning_rate': 1.6639682297235374e-05, 'epoch': 0.84}                                           
{'loss': 0.6525, 'learning_rate': 1.633419886971132e-05, 'epoch': 0.92}                                            
{'loss': 0.648, 'learning_rate': 1.6028715442187264e-05, 'epoch': 0.99}                                            
 20%|██████████████▌                                                          | 6546/32735 [02:33<10:39, 40.98it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 16
{'eval_loss': 0.6481423377990723, 'eval_accuracy': 0.615595826468973, 'eval_runtime': 2.605, 'eval_samples_per_second': 2097.145, 'eval_steps_per_second': 131.287, 'epoch': 1.0}                                                     
 20%|██████████████▌                                                          | 6547/32735 [02:36<10:38, 40.98it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-6547                                                 
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-6547/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-6547/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-6547/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-6547/special_tokens_map.json
{'loss': 0.6453, 'learning_rate': 1.5723232014663207e-05, 'epoch': 1.07}                                           
{'loss': 0.6492, 'learning_rate': 1.541774858713915e-05, 'epoch': 1.15}                                            
{'loss': 0.6525, 'learning_rate': 1.5112265159615092e-05, 'epoch': 1.22}                                           
{'loss': 0.6429, 'learning_rate': 1.4806781732091035e-05, 'epoch': 1.3}                                            
{'loss': 0.6481, 'learning_rate': 1.4501298304566977e-05, 'epoch': 1.37}                                           
{'loss': 0.6459, 'learning_rate': 1.4195814877042922e-05, 'epoch': 1.45}                                           
{'loss': 0.6458, 'learning_rate': 1.3890331449518865e-05, 'epoch': 1.53}                                           
{'loss': 0.6447, 'learning_rate': 1.3584848021994808e-05, 'epoch': 1.6}                                            
{'loss': 0.6446, 'learning_rate': 1.3279364594470752e-05, 'epoch': 1.68}                                           
{'loss': 0.6476, 'learning_rate': 1.2973881166946693e-05, 'epoch': 1.76}                                           
{'loss': 0.6486, 'learning_rate': 1.2668397739422638e-05, 'epoch': 1.83}                                           
{'loss': 0.6453, 'learning_rate': 1.236291431189858e-05, 'epoch': 1.91}                                            
{'loss': 0.6406, 'learning_rate': 1.2057430884374523e-05, 'epoch': 1.99}                                           
 40%|████████████████████████████▊                                           | 13090/32735 [05:09<07:35, 43.11it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 16
{'eval_loss': 0.6442179083824158, 'eval_accuracy': 0.6207212154493867, 'eval_runtime': 2.6047, 'eval_samples_per_second': 2097.391, 'eval_steps_per_second': 131.303, 'epoch': 2.0}                                                   
 40%|████████████████████████████▊                                           | 13094/32735 [05:12<07:35, 43.11it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-13094                                                
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-13094/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-13094/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-13094/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-13094/special_tokens_map.json
{'loss': 0.6433, 'learning_rate': 1.1751947456850468e-05, 'epoch': 2.06}                                           
{'loss': 0.6364, 'learning_rate': 1.144646402932641e-05, 'epoch': 2.14}                                            
{'loss': 0.639, 'learning_rate': 1.1140980601802354e-05, 'epoch': 2.21}                                            
{'loss': 0.6384, 'learning_rate': 1.0835497174278296e-05, 'epoch': 2.29}                                           
{'loss': 0.6442, 'learning_rate': 1.053001374675424e-05, 'epoch': 2.37}                                            
{'loss': 0.6412, 'learning_rate': 1.0224530319230184e-05, 'epoch': 2.44}                                           
{'loss': 0.6366, 'learning_rate': 9.919046891706126e-06, 'epoch': 2.52}                                            
{'loss': 0.6401, 'learning_rate': 9.613563464182069e-06, 'epoch': 2.6}                                             
{'loss': 0.6346, 'learning_rate': 9.308080036658012e-06, 'epoch': 2.67}                                            
{'loss': 0.6378, 'learning_rate': 9.002596609133956e-06, 'epoch': 2.75}                                            
{'loss': 0.6367, 'learning_rate': 8.697113181609899e-06, 'epoch': 2.83}                                            
{'loss': 0.6404, 'learning_rate': 8.391629754085842e-06, 'epoch': 2.9}                                             
{'loss': 0.6332, 'learning_rate': 8.086146326561785e-06, 'epoch': 2.98}                                            
 60%|███████████████████████████████████████████▏                            | 19641/32735 [07:45<05:24, 40.30it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 16
{'eval_loss': 0.6457436084747314, 'eval_accuracy': 0.6227347611202636, 'eval_runtime': 2.6039, 'eval_samples_per_second': 2097.981, 'eval_steps_per_second': 131.34, 'epoch': 3.0}                                                    
 60%|███████████████████████████████████████████▏                            | 19641/32735 [07:48<05:24, 40.30it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-19641                                                
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-19641/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-19641/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-19641/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-19641/special_tokens_map.json
{'loss': 0.6308, 'learning_rate': 7.780662899037727e-06, 'epoch': 3.05}                                            
{'loss': 0.6227, 'learning_rate': 7.47517947151367e-06, 'epoch': 3.13}                                             
{'loss': 0.629, 'learning_rate': 7.169696043989614e-06, 'epoch': 3.21}                                             
{'loss': 0.634, 'learning_rate': 6.8642126164655576e-06, 'epoch': 3.28}                                            
{'loss': 0.6341, 'learning_rate': 6.558729188941501e-06, 'epoch': 3.36}                                            
{'loss': 0.6255, 'learning_rate': 6.253245761417443e-06, 'epoch': 3.44}                                            
{'loss': 0.6312, 'learning_rate': 5.9477623338933865e-06, 'epoch': 3.51}                                           
{'loss': 0.633, 'learning_rate': 5.64227890636933e-06, 'epoch': 3.59}                                              
{'loss': 0.6305, 'learning_rate': 5.336795478845274e-06, 'epoch': 3.67}                                            
{'loss': 0.6286, 'learning_rate': 5.031312051321216e-06, 'epoch': 3.74}                                            
{'loss': 0.6345, 'learning_rate': 4.7258286237971595e-06, 'epoch': 3.82}                                           
{'loss': 0.6183, 'learning_rate': 4.420345196273103e-06, 'epoch': 3.89}                                            
{'loss': 0.6363, 'learning_rate': 4.114861768749045e-06, 'epoch': 3.97}                                            
 80%|█████████████████████████████████████████████████████████▌              | 26185/32735 [10:19<02:28, 44.24it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 16
{'eval_loss': 0.6473785042762756, 'eval_accuracy': 0.6220025626944902, 'eval_runtime': 2.6036, 'eval_samples_per_second': 2098.279, 'eval_steps_per_second': 131.358, 'epoch': 4.0}                                                   
 80%|█████████████████████████████████████████████████████████▌              | 26188/32735 [10:22<02:27, 44.24it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-26188                                                
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-26188/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-26188/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-26188/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-26188/special_tokens_map.json
{'loss': 0.628, 'learning_rate': 3.809378341224989e-06, 'epoch': 4.05}                                             
{'loss': 0.6201, 'learning_rate': 3.503894913700932e-06, 'epoch': 4.12}                                            
{'loss': 0.6248, 'learning_rate': 3.198411486176875e-06, 'epoch': 4.2}                                             
{'loss': 0.6247, 'learning_rate': 2.8929280586528187e-06, 'epoch': 4.28}                                           
{'loss': 0.6266, 'learning_rate': 2.5874446311287615e-06, 'epoch': 4.35}                                           
{'loss': 0.624, 'learning_rate': 2.2819612036047043e-06, 'epoch': 4.43}                                            
{'loss': 0.6265, 'learning_rate': 1.9764777760806476e-06, 'epoch': 4.51}                                           
{'loss': 0.6298, 'learning_rate': 1.670994348556591e-06, 'epoch': 4.58}                                            
{'loss': 0.634, 'learning_rate': 1.3655109210325341e-06, 'epoch': 4.66}                                            
{'loss': 0.6241, 'learning_rate': 1.0600274935084774e-06, 'epoch': 4.73}                                           
{'loss': 0.6235, 'learning_rate': 7.545440659844204e-07, 'epoch': 4.81}                                            
{'loss': 0.6252, 'learning_rate': 4.490606384603636e-07, 'epoch': 4.89}                                            
{'loss': 0.6214, 'learning_rate': 1.4357721093630674e-07, 'epoch': 4.96}                                           
100%|███████████████████████████████████████████████████████████████████████▉| 32731/32735 [12:54<00:00, 42.87it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 16
{'eval_loss': 0.6476873159408569, 'eval_accuracy': 0.6227347611202636, 'eval_runtime': 2.5708, 'eval_samples_per_second': 2125.008, 'eval_steps_per_second': 133.032, 'epoch': 5.0}                                                   
100%|████████████████████████████████████████████████████████████████████████| 32735/32735 [12:57<00:00, 42.87it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-32735                                                
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-32735/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-32735/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-32735/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-32735/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from tiny-bert-finetuned-qnli/checkpoint-19641 (score: 0.6227347611202636).
{'train_runtime': 777.5822, 'train_samples_per_second': 673.517, 'train_steps_per_second': 42.098, 'train_loss': 0.6397813038660658, 'epoch': 5.0}
100%|████████████████████████████████████████████████████████████████████████| 32735/32735 [12:57<00:00, 42.10it/s]


***** Running training *****
  Num examples = 104743
  Num Epochs = 5
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 16370
{'loss': 0.6824, 'learning_rate': 1.938912645082468e-05, 'epoch': 0.15}                                            
{'loss': 0.6643, 'learning_rate': 1.8778252901649362e-05, 'epoch': 0.31}                                           
{'loss': 0.6608, 'learning_rate': 1.816737935247404e-05, 'epoch': 0.46}                                            
{'loss': 0.6563, 'learning_rate': 1.755650580329872e-05, 'epoch': 0.61}                                            
{'loss': 0.6564, 'learning_rate': 1.69456322541234e-05, 'epoch': 0.76}                                             
{'loss': 0.6559, 'learning_rate': 1.6334758704948076e-05, 'epoch': 0.92}                                           
 20%|██████████████▌                                                          | 3274/16370 [01:43<07:05, 30.75it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 32
{'eval_loss': 0.6468596458435059, 'eval_accuracy': 0.615595826468973, 'eval_runtime': 2.0062, 'eval_samples_per_second': 2723.119, 'eval_steps_per_second': 85.238, 'epoch': 1.0}                                                     
 20%|██████████████▌                                                          | 3274/16370 [01:45<07:05, 30.75it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-3274                                                 
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-3274/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-3274/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-3274/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-3274/special_tokens_map.json
{'loss': 0.6477, 'learning_rate': 1.5723885155772757e-05, 'epoch': 1.07}                                           
{'loss': 0.6537, 'learning_rate': 1.5113011606597437e-05, 'epoch': 1.22}                                           
{'loss': 0.6472, 'learning_rate': 1.4502138057422115e-05, 'epoch': 1.37}                                           
{'loss': 0.648, 'learning_rate': 1.3891264508246794e-05, 'epoch': 1.53}                                            
{'loss': 0.6456, 'learning_rate': 1.3280390959071474e-05, 'epoch': 1.68}                                           
{'loss': 0.6506, 'learning_rate': 1.2669517409896153e-05, 'epoch': 1.83}                                           
{'loss': 0.6474, 'learning_rate': 1.2058643860720831e-05, 'epoch': 1.99}                                           
 40%|█████████████████████████████▏                                           | 6548/16370 [03:27<05:03, 32.34it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 32
{'eval_loss': 0.6444548964500427, 'eval_accuracy': 0.6203551162365001, 'eval_runtime': 2.0082, 'eval_samples_per_second': 2720.404, 'eval_steps_per_second': 85.153, 'epoch': 2.0}                                                    
 40%|█████████████████████████████▏                                           | 6548/16370 [03:29<05:03, 32.34it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-6548                                                 
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-6548/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-6548/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-6548/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-6548/special_tokens_map.json
{'loss': 0.6427, 'learning_rate': 1.1447770311545512e-05, 'epoch': 2.14}                                           
{'loss': 0.6416, 'learning_rate': 1.083689676237019e-05, 'epoch': 2.29}                                            
{'loss': 0.6477, 'learning_rate': 1.0226023213194869e-05, 'epoch': 2.44}                                           
{'loss': 0.6409, 'learning_rate': 9.615149664019549e-06, 'epoch': 2.6}                                             
{'loss': 0.6417, 'learning_rate': 9.004276114844227e-06, 'epoch': 2.75}                                            
{'loss': 0.6445, 'learning_rate': 8.393402565668908e-06, 'epoch': 2.9}                                             
 60%|███████████████████████████████████████████▊                             | 9821/16370 [05:12<03:29, 31.26it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 32
{'eval_loss': 0.6446802616119385, 'eval_accuracy': 0.6218195130880468, 'eval_runtime': 1.9947, 'eval_samples_per_second': 2738.769, 'eval_steps_per_second': 85.728, 'epoch': 3.0}                                                    
 60%|███████████████████████████████████████████▊                             | 9822/16370 [05:14<03:29, 31.26it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-9822                                                 
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-9822/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-9822/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-9822/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-9822/special_tokens_map.json
{'loss': 0.6358, 'learning_rate': 7.782529016493586e-06, 'epoch': 3.05}                                            
{'loss': 0.6358, 'learning_rate': 7.171655467318266e-06, 'epoch': 3.21}                                            
{'loss': 0.6402, 'learning_rate': 6.560781918142944e-06, 'epoch': 3.36}                                            
{'loss': 0.6358, 'learning_rate': 5.949908368967624e-06, 'epoch': 3.51}                                            
{'loss': 0.6377, 'learning_rate': 5.339034819792304e-06, 'epoch': 3.67}                                            
{'loss': 0.6393, 'learning_rate': 4.728161270616982e-06, 'epoch': 3.82}                                            
{'loss': 0.6348, 'learning_rate': 4.117287721441662e-06, 'epoch': 3.97}                                            
 80%|█████████████████████████████████████████████████████████▌              | 13096/16370 [06:57<01:39, 33.00it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 32
{'eval_loss': 0.6449214816093445, 'eval_accuracy': 0.6220025626944902, 'eval_runtime': 2.0349, 'eval_samples_per_second': 2684.629, 'eval_steps_per_second': 84.033, 'epoch': 4.0}                                                    
 80%|█████████████████████████████████████████████████████████▌              | 13096/16370 [06:59<01:39, 33.00it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-13096                                                
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-13096/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-13096/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-13096/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-13096/special_tokens_map.json
{'loss': 0.6327, 'learning_rate': 3.506414172266341e-06, 'epoch': 4.12}                                            
{'loss': 0.6312, 'learning_rate': 2.8955406230910206e-06, 'epoch': 4.28}                                           
{'loss': 0.6329, 'learning_rate': 2.2846670739156996e-06, 'epoch': 4.43}                                           
{'loss': 0.6363, 'learning_rate': 1.6737935247403788e-06, 'epoch': 4.58}                                           
{'loss': 0.6367, 'learning_rate': 1.0629199755650582e-06, 'epoch': 4.73}                                           
{'loss': 0.6324, 'learning_rate': 4.5204642638973736e-07, 'epoch': 4.89}                                           
100%|████████████████████████████████████████████████████████████████████████| 16370/16370 [08:42<00:00, 30.50it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5463
  Batch size = 32
{'eval_loss': 0.6452161073684692, 'eval_accuracy': 0.6232839099395936, 'eval_runtime': 2.0269, 'eval_samples_per_second': 2695.194, 'eval_steps_per_second': 84.364, 'epoch': 5.0}                                                    
100%|████████████████████████████████████████████████████████████████████████| 16370/16370 [08:45<00:00, 30.50it/sSaving model checkpoint to tiny-bert-finetuned-qnli/checkpoint-16370                                                
Configuration saved in tiny-bert-finetuned-qnli/checkpoint-16370/config.json
Model weights saved in tiny-bert-finetuned-qnli/checkpoint-16370/pytorch_model.bin
tokenizer config file saved in tiny-bert-finetuned-qnli/checkpoint-16370/tokenizer_config.json
Special tokens file saved in tiny-bert-finetuned-qnli/checkpoint-16370/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from tiny-bert-finetuned-qnli/checkpoint-16370 (score: 0.6232839099395936).
{'train_runtime': 525.2352, 'train_samples_per_second': 997.106, 'train_steps_per_second': 31.167, 'train_loss': 0.6446110645590137, 'epoch': 5.0}
100%|████████████████████████████████████████████████████████████████████████| 16370/16370 [08:45<00:00, 31.17it/s]
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzdleyWPP7Cu"
      },
      "source": [
        "## **Environment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CjP4IClSCp0"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg8fiDJORK09"
      },
      "source": [
        "## **Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8ICLU9dROrm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "dataset = load_dataset(\"glue\", 'qnli')\n",
        "metric = load_metric('glue', 'qnli')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXydFpKRSV6m"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM8cg-DNSXZc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['sentence'], truncation=True, padding=True, max_length=512, add_special_tokens = True)\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYdkKBZiTd3T"
      },
      "source": [
        "## **Fine-tune**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugdq4_u1TfHy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"tiny-bert-finetuned-qnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "tinybert.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "bc629e34c788e0240ac21804165d0276d4d32b0e138d69eebe2c3e1c6d84e9d9"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('tinybert': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
